\section{Conclusiones}
A lo largo del TP comprendimos que el problema de estimar la selectividad en una columna de una tabla no es trivial, sino que esconde todo un mundo de matemáticas, decisiones algoritmicas, de velocidad, de consumo de memoria de mayor o menor precisión. Implementamos diversos estimadores recomendados y diseñamos uno propio que logró mejor perfomance en la mayoría de las distribuciones provistas por la cátedra.

Analizamos la performance de los estimadores con diferentes distribuciones de datos, y concluimos que no hay un claro ganador para todas las distribuciones, sino que cada uno presenta ventajas y desventajas y que hace falta conocer el dominio en el que se van a utilizar para sacarles todo el jugo posible.

Descubrimos también que un estimador ``perfecto'' trae asociado siempre un gran costo, ya sea en velocidad o en memoria (o ambas), y que ni siquiera merece ser llamado estimador.

La mayor parte de nuestras predicciones teóricas se verificaron en la práctica, pero encontramos una situación curiosa según la cual aumentar en exceso un parámetro que pensábamos estaba directamente asociado con la ``precisión'' terminaba repercutiendo negativamente en la performance del estimador, dada su implementación algorítmica.

Como conclusión general de nuestras comparaciones, sostenemos que nuestro estimador se comporta mejor que los demás en el caso general, al menos para las distribuciones que analizamos, y preferentemente con un $S$ lo más grande que la memoria disponible permita. Solo priorizaríamos la elección de Steps si sabemos de antemano que las consultas presentarán más desigualdades que igualdades.