\section{Discusión}
Basándonos en las pruebas realizadas y corroborando más precisamente con el cuadro \ref{tab:hresult} nos vemos tentados de decir que el estimador propuesto por nuestro grupo es un buen candidato a ser elegido como el mejor dada una distribución desconocida. Esto último se debe a que el cálculo de la estimación deja de ser ``estimada'' para pasar a ser exacta. Sin embargo, asegurar esto sería un error, dado que dicho estimador abusa de la posibilidad de almacenar todos los elementos distintos en memoria y se toma un tiempo eventualmente grande para calcular la selectividad por mayor en algunos casos, con lo cual no podría ser implementado en situaciones donde la velocidad es más crítica que la precisión, o donde sabemos con certeza que la distribución abarcara una gran cantidad de posibilidades distintas. Dicho esto, lo primero que podemos comentar es que en caso en que podamos hacer uso de la memoria para guardar los elementos distintos y que podamos aceptar un tiempo de consulta lineal en el rango de la distribución, definitivamente nustro estimador sería la mejor elección posible.

Igualmente sabemos que existen casos en los que no se podrá realizar tal acción y por eso seguiremos hablando sobre las comparaciones detalladas entre las implementaciones de los estimadores presentes en el paper de Piatetsky-Shapiro.

Basándonos en nuestro análisis empírico podemos afirmar que ambas funcionan de manera satisfactoria para distribuciones del tipo uniformes y normales. Esto no debería ser ninguna sorpresa para nosotros, dado que ya fue anticipado por qué debía pasar en la sección \textbf{3}. Sin embargo, un detalle que no preevimos fue el comportamiento oscilatorio de las comparaciones por mayor del estimador \textbf{Steps} para valores grandes de $S$. Como ya comentamos, este fenómeno que no previmos en el análisis teórico se debe a que al agrandarse la cantidad de buckets aumenta significativamente la cantidad de elementos que coinciden con dos o más límites entre buckets (ver sección 2.2, 4.4.1 y 4.4.2).

Mirando \ref{tab:hresult} podemos notar que el estimador \textbf{Classic} suele comportarse muy bien en \textbf{todas} las distribuciones de las posibles columnas de la base otorgada por la cátedra a la hora de consultar por igualdad. No ocurre lo mismo cuando la consulta es de selectividad por mayor, como ya fue adelantado por nosotros en diversos puntos. De hecho se ``invierten'' los papeles y quien logra mejores resultados es el estimador \textbf{Steps}. Igualmente, dado que existen muchos empates en este caso (en la comparación entre los dos tipos de estimadores) creemos que la ventaja no contrarresta la diferencia que obtiene \textbf{Classic} en la comparación por igual. Con lo cual para el caso general si tuviésemos que elegir uno de los dos para tomarlo como estimador sin saber de antemano qué tipo de consultas será la más frecuente, elegiríamos a \textbf{Classic}.

Como agregado, las tres implementaciones (y volvemos a sumar a nuestro estimador) tardan tiempo en inicializarse, pero la idea es que cuando alguien realice una consulta a la base demoren muy poco tiempo en devolver una respuesta que se considere correcta. Es necesario que sea rápido dada que se hacen muchas consultas SQL por unidad de tiempo en un servidor. En cambio la creación de cero de un estimador no es tan frecuente, y se pueden aprovechar las noches (o los momentos de menos tráfico) para actualizarlo. 
